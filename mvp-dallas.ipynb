{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVI & COVID - DALLAS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "The CDC's social vulnerability index (SVI) is a scale that predicts the vulnerability of a population in the event of an emergency or natural disaster. COVID is the first global pandemic since the development of this measure. We will evaluate the association between SVI score and COVID case count in Dallas, Texas. Features from this measure will be incorporated into a predictive model that can be used to guide recovery resource prioritization.\n",
    "\n",
    "\n",
    "\n",
    "**Goals**      \n",
    "1. Evaluate association between SVI score and COVID case count in Dallas, TX     \n",
    "2. Build a model based SVI score component features that can predict COVID cases by census tract within Dallas, TX\n",
    "4. Is there a difference between San Antonio and Dallas results?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wrangle'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-88e32550ac1c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mwrangle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mexplore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodel_MAE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'wrangle'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import wrangle\n",
    "import explore\n",
    "import model_MAE\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from statsmodels.formula.api import ols\n",
    "from sklearn.metrics import mean_squared_error, r2_score, explained_variance_score\n",
    "from sklearn.feature_selection import f_regression, SelectKBest, RFE \n",
    "from sklearn.linear_model import LinearRegression, LassoLars, TweedieRegressor\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# Classfication Modeling:\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix, classification_report\n",
    "import model_classification\n",
    "\n",
    "from math import sqrt\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Acquire & Prepare\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, train_exp, X_train_scaled, y_train, X_test_scaled, y_test = wrangle.wrangle_dallas_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore\n",
    "\n",
    "Exploration focuses on answering questions regarding the relationship between the CDC's range category SVI score and cases of COVID-19 per 100k.\n",
    "\n",
    "- Visualize cases per 100K by binned SVI value\n",
    "    - Appear to be distinct\n",
    "    - Will conduct parametric ANOCA (Kruskal) test to confirm\n",
    "- Verify raw SVI score relationship to cases per 100K\n",
    "    - will conduct Pearson's R correlation test\n",
    "- Explore distribution of casses per 100K with SVI score\n",
    "- Explore distribution of flags by SVI score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis Testing\n",
    "\n",
    "### Question One: Is there a correlation between the CDC's Range Category SVI Score and COVID-19 Infection Cases per 100k Individuals?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explore.sns_boxplot(train_exp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Takeaway:**\n",
    "`There appears to be a correlation between COVID-19 Count and SVI Category. Next step is Hypothesis testing between categories to validate statistical significance`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean COVID-19 Count By CDC's SVI Category\n",
    "All = round(train_exp.tract_cases_per_100k.mean(),5)\n",
    "low = round((train_exp[train_exp.bin_svi == 'Low']).tract_cases_per_100k.mean(),5)\n",
    "low_mod = round((train_exp[train_exp.bin_svi == 'Low Moderate']).tract_cases_per_100k.mean(),6)\n",
    "mod_high = round((train_exp[train_exp.bin_svi == 'Moderate High']).tract_cases_per_100k.mean(),6)\n",
    "high = round((train_exp[train_exp.bin_svi== 'High']).tract_cases_per_100k.mean(),6)\n",
    "\n",
    "print(f'The average number of cases per 100k for all CDC SVI Range Categories is {All}') \n",
    "print(f'The average number of cases per 100k for CDC SVI Range Category (low) is {low}')\n",
    "print(f'The average number of cases per 100k for CDC SVI Range Category (low_mod) is {low_mod}')\n",
    "print(f'The average number of cases per 100k for CDC SVI Range Category (mod_high) is {mod_high}')\n",
    "print(f'The average number of cases per 100k for CDC SVI Range Category (high) is {high}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low = (train_exp[train_exp.bin_svi == 'Low']).tract_cases_per_100k\n",
    "low_mod = (train_exp[train_exp.bin_svi == 'Low Moderate']).tract_cases_per_100k\n",
    "mod_high = (train_exp[train_exp.bin_svi == 'Moderate High']).tract_cases_per_100k\n",
    "high = (train_exp[train_exp.bin_svi== 'High']).tract_cases_per_100k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variance Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.levene(low, low_mod, mod_high, high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.01\n",
    "null = \"Average number of COVID-19 cases per 100k is the same across all CDC SVI Range Categories \"\n",
    "alternate = \"Average number of COVID-19 cases per 100k is significantly different across all CDC SVI Range Categories \"\n",
    "explore.kruskal_test(low, low_mod, mod_high, high, null, alternate, alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Takeaway:**\n",
    "`We can state with 99% certainty that there is a statistically significant difference between all of the CDC SVI Range Categories`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question Two: Is there a correlation between raw_svi and cases per 100k?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_svi = train_exp.raw_svi\n",
    "cases_per_100k = train_exp.tract_cases_per_100k\n",
    "alpha = 0.01\n",
    "null = \"There is no statistically significant difference betweeen raw_svi and cases per 100K \"\n",
    "alternate = \"There is a statistically significant difference betweeen raw_svi and cases per 100K\"\n",
    "explore.pearson(raw_svi, cases_per_100k, null, alternate, alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Takeaway:**\n",
    "`We can state with 99% certainty that there is not a statistically significant difference between the Social Vulnerability Index and Census Tract cases per 100,000 people.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explore.joint_plot_index('raw_svi','tract_cases_per_100k', train_exp, 'bin_svi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explore.my_plotter(train_exp, \"all_flags_total\", \"All SVI Component Flags\", \"tract_cases_per_100k\", \"Cases by Tract per 100k\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explore.hist_case_title(train_exp.tract_cases_per_100k, \"Distribution of Cases in Dallas, TX: December 8th 2020\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore: Feature Engineering (Clustering)\n",
    "\n",
    "`Features that demonstrate potential for Clustering`\n",
    "\n",
    "1. E_POV (Persons below poverty estimate)\n",
    "2. EP_POV (Percentage of persons below poverty estimate)\n",
    "3. SPL_THEME1 (Sum of series for Socioeconomic theme)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scatterplot spl_theme1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explore.cluster_scatter(train_exp, \n",
    "                'Sum of Flags for Socioeconomic Themes by Number of Cases per 100k', \n",
    "                'spl_theme1',\n",
    "                'Sum of Flags for Socioeconomic Themes',\n",
    "                'tract_cases_per_100k',\n",
    "                'Number of Cases per 100,000')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scatterplot e_pov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explore.cluster_scatter(train_exp, \n",
    "                'Persons Below Poverty Estimate by Number of Cases per 100k', \n",
    "                'e_pov',\n",
    "                'Persons Below Poverty Estimate',\n",
    "                'tract_cases_per_100k',\n",
    "                'Number of Cases per 100,000')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scatterplot ep_pov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explore.cluster_scatter(train_exp, \n",
    "                'Percentage of Persons Below Poverty Estimate by Number of Cases per 100k', \n",
    "                'ep_pov',\n",
    "                'Percentage of Persons Below Poverty Estimate',\n",
    "                'tract_cases_per_100k',\n",
    "                'Number of Cases per 100,000')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Poverty Cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Elbow Method to establish k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_vars = ['spl_theme1_scaled', 'ep_pov_scaled', 'e_pov_scaled']\n",
    "explore.elbow_plot(X_train_scaled, cluster_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_clusters, kmeans = explore.run_kmeans(train_exp, X_train_scaled, k=4, cluster_vars=cluster_vars, cluster_col_name = 'poverty_cluster')\n",
    "test_clusters = explore.kmeans_transform(X_test_scaled, kmeans, cluster_vars, cluster_col_name = 'poverty_cluster')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids = explore.get_centroids(cluster_vars, cluster_col_name='poverty_cluster', kmeans= kmeans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Append Cluster and Join Centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_exp = explore.add_to_train(train_clusters, centroids, train_exp, cluster_col_name = 'poverty_cluster')\n",
    "X_train_scaled = explore.add_to_train(train_clusters, centroids, X_train_scaled, cluster_col_name = 'poverty_cluster')\n",
    "X_test_scaled = explore.add_to_train(test_clusters, centroids, X_test_scaled, cluster_col_name = 'poverty_cluster')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Are the clusters significant?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explore.sns_boxplot_hypothesis(train_exp.poverty_cluster, \n",
    "                       train_exp.tract_cases_per_100k, \n",
    "                       \"Poverty Clusters\", \n",
    "                       \"Cases per 100k\", \n",
    "                       \"Are The Clusters Significant?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hypothesis Testing: (ANOVA/Kruskal)\n",
    "\n",
    "Is there a statistically significant difference between poverty_clusters and cases per 100k?\n",
    "\n",
    "Null Hypothesis: Mean # of cases is the same across all clusters\n",
    "\n",
    "Alternative Hypothesis: Mean # of cases is different across clusters\n",
    "\n",
    "alpha=0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_0 = train_exp[train_exp.poverty_cluster == 0].tract_cases_per_100k\n",
    "cluster_1 = train_exp[train_exp.poverty_cluster == 1].tract_cases_per_100k\n",
    "cluster_2 = train_exp[train_exp.poverty_cluster == 2].tract_cases_per_100k\n",
    "cluster_3 = train_exp[train_exp.poverty_cluster == 3].tract_cases_per_100k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variance Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.levene(cluster_0, cluster_1, cluster_2, cluster_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.01\n",
    "null = \"Mean number of cases per 100k is the same across all clusters\"\n",
    "alternate = \"Average number of cases per 100k is significantly different between clusters\"\n",
    "explore.kruskal_test(low, low_mod, mod_high, high, null, alternate, alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split clusters in to dummy variables for modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled = pd.get_dummies(X_train_scaled,\n",
    "                           columns=[\"poverty_cluster\"])\n",
    "X_test_scaled = pd.get_dummies(X_test_scaled,\n",
    "                           columns=[\"poverty_cluster\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model the Data\n",
    "\n",
    "- Baseline for modeling determined by plotting the histogram distribution of COVID-19 cases per 100k.\n",
    "- The skew observed in the distribution led us to use the median for this value instead of the mean??\n",
    "- Used cross validation due to limited size of dataset. Size of dataset limited by Dallas number of census tracts.\n",
    "- Three of the 4 models used all of the features in the dataset, one model used only the top4 features identified by RFE.\n",
    "- Linear Regression, LassoLars, and 2 degree polynomial features used all features and a 2nd version of 2 degree polynomial was run with just the top4 features.\n",
    "- Of these the LassoLars had the least MAE (mean absolute error) and was run on out of sample data (test).\n",
    "- This model had nearly identical MAE when run on out of sample data, only a 0.7 difference in MAE.\n",
    "- Overall this is a 25% improvement from mean baseline MAE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is the mean vs median of the target variable?\n",
    "y_train.tract_cases_per_100k.mean(), y_train.tract_cases_per_100k.median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the mean absolute error (MAE) of the baseline using mean\n",
    "mean_baseMAE, basepred1 = model_MAE.get_baseline_mean(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Ranking\n",
    "\n",
    "- Use recursive feature elimination to evaluate features for modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rankdf = model_MAE.feature_ranking(X_train_scaled, y_train)\n",
    "rankdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only raw svi score\n",
    "X_raw_svi = X_train_scaled[['raw_svi']]\n",
    "# binned svi score by CDC range category = 1st ranked\n",
    "X_rank_svi_only = X_train_scaled[['rank_svi_scaled']]\n",
    "# top 4 ranked features\n",
    "X_top4 = X_train_scaled[['centroid_ep_pov_scaled', 'centroid_e_pov_scaled', 'centroid_spl_theme1_scaled', 'r_status_fall']]\n",
    "# only the summary of the flags = 19th ranked\n",
    "X_all_flags_only = X_train_scaled[['all_flags_total_scaled']]\n",
    "# only summary flags, should be the same as all flags total? = 5th, 12th, 15th, 21st\n",
    "X_summary_flags = X_train_scaled[['f_comp_total_scaled', 'f_soci_total_scaled', 'f_status_total_scaled', 'f_trans_total_scaled']]\n",
    "# all individual flags\n",
    "X_not_summary_flags = X_train_scaled[['f_nohsdp_soci', 'f_minrty_status', 'f_groupq_trans', 'f_unemp_soci', \n",
    "                                     'f_disabl_comp', 'f_noveh_trans', 'f_mobile_trans', 'f_age65_comp', \n",
    "                                     'f_age17_comp', 'f_pov_soci', 'f_limeng_status', 'f_crowd_trans', \n",
    "                                      'f_pci_soci', 'f_sngpnt_comp', 'f_munit_trans']]\n",
    "# top 10 based on RFE\n",
    "X_top10 = X_train_scaled[['centroid_ep_pov_scaled', 'centroid_e_pov_scaled', 'centroid_spl_theme1_scaled', 'r_status_fall',\n",
    "                         'delta', 'avg3yr', 'rank_svi_scaled', 'e_pov_scaled', 'ep_pov_scaled', 'f_nohsdp_soci']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Models\n",
    "\n",
    "- due to limited size of dataset (limited by number of zip codes in Dallas county) cross validation will be used for the train and validate stages of modeling\n",
    "- regression models will be used because the target variable is continuous\n",
    "- models to try: linear regression, LassoLars, Tweedie Regressor Random Forest Regressor, Support Vector Regressor (SVR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create variables for loop\n",
    "df2test = [X_rank_svi_only, X_top4, X_all_flags_only, X_summary_flags, X_not_summary_flags, X_train_scaled, \n",
    "           X_raw_svi, X_top10]\n",
    "target = y_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression Models\n",
    "cvlm_MAE_list = []\n",
    "for df in df2test:\n",
    "    cvlm_MAE = model_MAE.cvLinearReg(df, target) \n",
    "    cvlm_MAE_list.append(cvlm_MAE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LassoLars Models\n",
    "cvll_MAE_list = []\n",
    "for df in df2test:\n",
    "    cvll_MAE = model_MAE.cvLassoLars(df, target, 1) \n",
    "    cvll_MAE_list.append(cvll_MAE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Models\n",
    "cvrf_MAE_list = []\n",
    "for df in df2test:\n",
    "    cvrf_MAE = model_MAE.cvRandomForest(df, target, 4) \n",
    "    cvrf_MAE_list.append(cvrf_MAE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tweedie Regressor Models\n",
    "cvtw_MAE_list = []\n",
    "for df in df2test:\n",
    "    cvtw_MAE = model_MAE.cvTweedie(df, target, 1.5, .5)\n",
    "    cvtw_MAE_list.append(cvtw_MAE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support Vector Models\n",
    "cvSVRrbf_MAE_list = []\n",
    "for df in df2test:\n",
    "    cvSVRrbf_MAE = model_MAE.cvSVR(df, target, 'rbf')\n",
    "    cvSVRrbf_MAE_list.append(cvSVRrbf_MAE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support Vector Models\n",
    "cvSVRlinear_MAE_list = []\n",
    "for df in df2test:\n",
    "    cvSVRlinear_MAE = model_MAE.cvSVR(df, target, 'linear')\n",
    "    cvSVRlinear_MAE_list.append(cvSVRlinear_MAE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpret the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe for results of all train models\n",
    "df_list = ['rank_svi_only', 'top4', 'total_all_flags_only', 'summary_flags', 'not_summary_flags', 'all_features', \n",
    "           'raw_svi_only', 'top10']\n",
    "\n",
    "results = pd.DataFrame(df_list, columns=['Features'])\n",
    "results['Base_mean_MAE'] = mean_baseMAE\n",
    "results['LinearRegression_MAE'] = cvlm_MAE_list\n",
    "results['LassoLars_MAE'] = cvll_MAE_list\n",
    "results['Tweedie_MAE'] = cvtw_MAE_list\n",
    "results['RandomForest_MAE'] = cvrf_MAE_list\n",
    "results['SVR_rbf_MAE'] = cvSVRrbf_MAE_list\n",
    "results['SVR_linear_MAE'] = cvSVRlinear_MAE_list\n",
    "results.sort_values('LinearRegression_MAE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create test dataframe with only Top10 features as identified by RFE as that is the best performing model\n",
    "X_test_top10 = X_test_scaled[['centroid_ep_pov_scaled', 'centroid_e_pov_scaled', 'centroid_spl_theme1_scaled', 'r_status_fall',\n",
    "                         'delta', 'avg3yr', 'rank_svi_scaled', 'e_pov_scaled', 'ep_pov_scaled', 'f_nohsdp_soci']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "LRtestMAE, modelLR = model_MAE.linear_test(X_top10, y_train, X_test_top10, y_test)\n",
    "LRtestMAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TWtestMAE, modelTW = model_MAE.tweedie_test(X_top10, y_train, X_test_top10, y_test, 1.5, .5)\n",
    "TWtestMAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LLtestMAE, modelLL = model_MAE.lasso_lars_test(X_top10, y_train, X_test_top10, y_test)\n",
    "LLtestMAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top4 features as identified by RFE seem to be overfitted? try top4?\n",
    "X_test_top4 = X_test_scaled[['centroid_ep_pov_scaled', 'centroid_e_pov_scaled', 'centroid_spl_theme1_scaled', 'r_status_fall']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "LRtestMAE, modelLR = model_MAE.linear_test(X_top4, y_train, X_test_top4, y_test)\n",
    "LRtestMAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TWtestMAE, modelTW = model_MAE.tweedie_test(X_top4, y_train, X_test_top4, y_test, 1.5, .5)\n",
    "TWtestMAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LLtestMAE, modelLL = model_MAE.lasso_lars_test(X_top4, y_train, X_test_top4, y_test)\n",
    "LLtestMAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Report Metrics in Context    \n",
    "\n",
    "- All models performed worse than baseline on test\n",
    "- SVI and features do not seem to correlate as closely with high case rates in Dallas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LassoLars Summary:\n",
    "\n",
    "LASSO = Least Absolute Shrinkage and Selection Operator\n",
    "LARS = Least Angle Regression\n",
    "\n",
    "LASSO + LARS performs both feature selection (LASSO) and noise reduction within the same model. If the target variable (Y) is normally distributed. As alpha increases, the efficiency of the model features taper off as indicated in the graph. The features only add a fixed amount of benefit to the model, but once alpha reaches a certain point, the benefit derived from a particular features plateaus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LASSOLARS Results table:\n",
    "ll_result = pd.DataFrame()\n",
    "x_train_columns = X_test_top4.columns.tolist()\n",
    "ll_result['features'] = x_train_columns\n",
    "ll_result['coefs'] = modelLL.coef_\n",
    "ll_result['abs_coefs'] = abs(modelLL.coef_)\n",
    "ll_result.sort_values(by = 'abs_coefs', ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weaknesses\n",
    "\n",
    "- There is no consensus in research literature as to which components of the SVI have the strongest correlation to health risks or outcomes. There are some demographic groups requiring careful interpretation of the results due to their unique characteristics.\n",
    "- These models are not intended in any way to be presented as predictions of infection; the medical reasons for COVID transmission are still as yet undetermined, and would require a vastly more complex model than is presented here. The purpose of this model was to measure and predict those areas most in need of help and response from communities and local officials. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bring in classification datasets with new y variable\n",
    "class_df, class_train_exp, class_X_train_scaled, class_y_train, class_X_test_scaled, class_y_test = wrangle.wrangle_dallas_data_class()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for nulls in dataframe:\n",
    "class_df[class_df['rank_cases'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only raw svi score\n",
    "cX_raw_svi = class_X_train_scaled[['raw_svi']]\n",
    "# binned svi score by CDC range category = 1st ranked\n",
    "cX_rank_svi_only = class_X_train_scaled[['rank_svi_scaled']]\n",
    "# top 4 ranked features\n",
    "cX_top4 = class_X_train_scaled[['spl_theme1_scaled', 'r_status_fall', 'delta', 'avg3yr']]\n",
    "# only the summary of the flags = 19th ranked\n",
    "cX_all_flags_only = class_X_train_scaled[['all_flags_total_scaled']]\n",
    "# only summary flags, should be the same as all flags total? = 5th, 12th, 15th, 21st\n",
    "cX_summary_flags = class_X_train_scaled[['f_comp_total_scaled', 'f_soci_total_scaled', 'f_status_total_scaled', 'f_trans_total_scaled']]\n",
    "# all individual flags\n",
    "cX_not_summary_flags = class_X_train_scaled[['f_nohsdp_soci', 'f_minrty_status', 'f_groupq_trans', 'f_unemp_soci', \n",
    "                                     'f_disabl_comp', 'f_noveh_trans', 'f_mobile_trans', 'f_age65_comp', \n",
    "                                     'f_age17_comp', 'f_pov_soci', 'f_limeng_status', 'f_crowd_trans', \n",
    "                                      'f_pci_soci', 'f_sngpnt_comp', 'f_munit_trans']]\n",
    "# top 10 by RFE\n",
    "cX_top10 = class_X_train_scaled[['spl_theme1_scaled', 'r_status_fall', 'delta', 'avg3yr', 'rank_svi_scaled', \n",
    "                          'f_soci_total_scaled', 'f_pov_soci', 'ep_pov_scaled', 'raw_svi', 'f_age17_comp']]\n",
    "# engineered features only\n",
    "cX_svifeatures = class_X_train_scaled[['rising', 'falling', 'delta', 'avg3yr', 'r_soci_rise', 'r_comp_rise', \n",
    "                                'r_status_rise', 'r_trans_rise', 'r_soci_fall', 'r_comp_fall', 'r_status_fall', 'r_trans_fall']]\n",
    "cX_Rlist = class_X_train_scaled[['rank_svi_scaled', 'rising', 'falling', 'delta', 'avg3yr']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create variables for loop\n",
    "cdf2test = [cX_rank_svi_only, cX_top4, cX_all_flags_only, cX_summary_flags, cX_not_summary_flags, class_X_train_scaled, \n",
    "           cX_raw_svi, cX_top10, cX_svifeatures, cX_Rlist]\n",
    "ctarget = class_y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_y_train.rank_cases.value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_df.tract_cases_per_100k.plot(kind = 'hist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline model we have chosen is the most commonly occuring case bin, rank 3, at 56%. \n",
    "\n",
    "- This means, if our model returns an accuracy that is > 56%, our model is better at predicting COVID case count than simply choosing the most commonly occuring bin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_classification.random_forest_class(cX_raw_svi, ctarget, max_depth = 4, n_estimators = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_X_train_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create variables for loop\n",
    "crf2test = [\n",
    "            cX_rank_svi_only, \n",
    "            cX_top4, \n",
    "            cX_all_flags_only, \n",
    "            cX_summary_flags, \n",
    "            cX_not_summary_flags, \n",
    "            class_X_train_scaled, \n",
    "            cX_raw_svi, \n",
    "            cX_top10, \n",
    "            cX_svifeatures, \n",
    "            cX_Rlist\n",
    "           ]\n",
    "# target var:\n",
    "ctarget = class_y_train\n",
    "\n",
    "\n",
    "# Random Forest Models Classification Loop\n",
    "cvrf_class_list = []\n",
    "for df in crf2test:\n",
    "    cvrf_class = model_classification.random_forest_class(df, ctarget, max_depth = 3, n_estimators = 75) \n",
    "    cvrf_class_list.append(cvrf_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Takeaways:\n",
    "\n",
    "- Best models performed just under the baseline, at 56%.\n",
    "- Interestingly, in the case of Dallas, the best Random Forest models used all the feature sets that contained all the features in our data. Even more interesting is that the summary flags and the features that are everything but the summary flags (which taken together is all the possible features) performed all the same.\n",
    "- For next steps, I would recommend that we adjust the binning of the case count column to more appropriately match the distribution of each county. We could run some statistical tests to determine what the optimal binning breakdown would be, and then programatically adjust the binning code to compensate for each distribution by county."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_classification.knn_classification(cX_rank_svi_only, class_y_train, n_neighbors=5, cv = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create variables for loop\n",
    "cknn2test = [\n",
    "            cX_rank_svi_only, \n",
    "            cX_top4, \n",
    "            cX_all_flags_only, \n",
    "            cX_summary_flags, \n",
    "            cX_not_summary_flags, \n",
    "            class_X_train_scaled, \n",
    "            cX_raw_svi, \n",
    "            cX_top10, \n",
    "            cX_svifeatures, \n",
    "            cX_Rlist\n",
    "           ]\n",
    "# target var:\n",
    "cknn_target = class_y_train\n",
    "\n",
    "\n",
    "# KNN Models Classification Loop\n",
    "cvknn_class_list = []\n",
    "for df in cknn2test:\n",
    "    cvknn_class = model_classification.knn_classification(df, cknn_target, n_neighbors=5, cv = 10) \n",
    "    cvknn_class_list.append(cvknn_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Takeaways:\n",
    "- Three models tied for first with using KNN. The feature sets used were the top 4 model, top 10 (which would obviously include the features in the top 4 model), and the summary flags feature set.\n",
    "- The summary flags feature set basically includes the count of all the flags in each sub-group of the SVI. Each individual flag was either a 0 or 1 for each row (census tract) based upon the percentage of the tract's population fitting into a given subgroup. Each of those flags were grouped into 3 gubgroups, and the feature set we used for one of the models used just those subgroup total flags (scaled).\n",
    "- \n",
    "- Since the accuracy of all the KNN Models did not exceed that of the Random Forest, we opted to take the Random Forest model into testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best model was top four, so we test on unseen data using that model.\n",
    "\n",
    "X_test_top_4_class = class_X_test_scaled[['spl_theme1_scaled', 'r_status_fall', 'delta', 'avg3yr']]\n",
    "raw_test_svi = class_X_test_scaled[['rank_svi_scaled']] \n",
    "class_y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_classification.rf_test_class(cX_rank_svi_only, ctarget, raw_test_svi, class_y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Modeling Summary\n",
    "\n",
    "- We ended up beating the Dallas baseline with our Classification models by 1%, at 57% accuracy. \n",
    "- Pretty much in line with the Regression Models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Steps: What Can We Do Now?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Is Dallas different from other cities?\n",
    "\n",
    "- Dallas is very different from San Antonio\n",
    "- SVI and features do not seem to correlate as closely with high case rates in Dallas (unlike in San Antonio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "- SVI trend for the county\n",
    "    - is rising? is declining? \n",
    "    - delta of SVI change year over year?\n",
    "    - std dev of SVI?\n",
    "\n",
    "- This is an area for possible further exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
